{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56391f70",
   "metadata": {},
   "source": [
    "## Data for Admission in the University: analisando modelos de Redes Neurais aplicados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6573f028",
   "metadata": {},
   "source": [
    "#### Importando as bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1531,
   "id": "d983fe89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d81340",
   "metadata": {},
   "source": [
    "#### Lendo o arquivo .csv com os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1532,
   "id": "297f5d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('adm_data.csv')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328c0e39",
   "metadata": {},
   "source": [
    "#### Renomeando colunas para tirar espaços desnecessários e facilitar o acesso a elas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1533,
   "id": "991dd87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {'Chance of Admit ': 'Chance of Admit'}, inplace = True)\n",
    "data.rename(columns = {'LOR ': 'LOR'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaa7733",
   "metadata": {},
   "source": [
    "#### Visualizando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1534,
   "id": "925a64b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR  CGPA  \\\n",
       "0           1        337          118                  4  4.5  4.5  9.65   \n",
       "1           2        324          107                  4  4.0  4.5  8.87   \n",
       "2           3        316          104                  3  3.0  3.5  8.00   \n",
       "3           4        322          110                  3  3.5  2.5  8.67   \n",
       "4           5        314          103                  2  2.0  3.0  8.21   \n",
       "\n",
       "   Research  Chance of Admit  \n",
       "0         1             0.92  \n",
       "1         1             0.76  \n",
       "2         1             0.72  \n",
       "3         1             0.80  \n",
       "4         0             0.65  "
      ]
     },
     "execution_count": 1534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61ec582",
   "metadata": {},
   "source": [
    "#### Retirando a coluna 'Serial No.', que não será utilizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1535,
   "id": "d93ccd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.drop(columns = ['Serial No.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bfaadd",
   "metadata": {},
   "source": [
    "#### Definindo features e label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1536,
   "id": "eebb1504",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['Chance of Admit']\n",
    "features = list(set(data1.columns).difference(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1537,
   "id": "c3c2bb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GRE Score',\n",
       " 'SOP',\n",
       " 'Research',\n",
       " 'CGPA',\n",
       " 'LOR',\n",
       " 'University Rating',\n",
       " 'TOEFL Score']"
      ]
     },
     "execution_count": 1537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6047ba7",
   "metadata": {},
   "source": [
    "#### Definindo X e y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1538,
   "id": "04f8ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[features]\n",
    "y = data[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c494d4fb",
   "metadata": {},
   "source": [
    "#### Split dos dados em train e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1539,
   "id": "9cf9ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 25\n",
    "# Aqui o shuffling dos índices é automático.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbea030d",
   "metadata": {},
   "source": [
    "#### Scaling nos dados para obter features no intervalo [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1540,
   "id": "227a1f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = (X_train - X_train.min())/(X_train.max() - X_train.min())\n",
    "X_test_scaled = (X_test - X_train.min())/(X_train.max() - X_train.min())\n",
    "# y_train e y_test já estão na escala correta, pois são probabilidades.\n",
    "y_train_scaled = y_train\n",
    "y_test_scaled = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ca288c",
   "metadata": {},
   "source": [
    "#### Criando funções para nos ajudar a definir e treinar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1541,
   "id": "32348741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(first_layer_units = 32, alpha = 0.1, reg_lambda = 0.1, features = [], random_seed = 0):\n",
    "    '''Create a defined model with learning rate = alpha.'''\n",
    "    # Defining Tensorflow's random seed to generate reproducible results.\n",
    "    tf.keras.utils.set_random_seed(random_seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "    \n",
    "    # We will be using Tensorflow's Sequencial API to build our model.\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # Adding first layer with input_dim = number of features and relu activation function \n",
    "    # to learn some non-linearities. We will also add L2 regularization in layer's kernel (weights).\n",
    "    model.add(tf.keras.layers.Dense(units = first_layer_units, input_shape = (len(features), ), \n",
    "                                   kernel_regularizer = tf.keras.regularizers.L2(l2 = reg_lambda),\n",
    "                                   activation = 'relu'))\n",
    "    \n",
    "    # Adding output layer with linear activation (linear regression output).\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(units = 1, activation = 'linear'))\n",
    "    \n",
    "    # Compiling model: selecting optimizer, loss and metrics.\n",
    "    \n",
    "    model.compile(optimizer = tf.keras.optimizers.RMSprop(learning_rate = alpha),\n",
    "                 loss = 'mean_squared_error',\n",
    "                 metrics = tf.keras.metrics.MeanSquaredError())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1542,
   "id": "a1766c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model = None, X_train = None, y_train = None, epochs = 10,\n",
    "               batch_size = 10, validation_split = 0.3,\n",
    "               random_seed = 0):\n",
    "    tf.keras.utils.set_random_seed(random_seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "    history = model.fit(x = X_train, y = y_train, batch_size = batch_size,\n",
    "         epochs = epochs, shuffle = False, \n",
    "         validation_split = validation_split, verbose = 0)\n",
    "    \n",
    "    hist = pd.DataFrame(history.history)\n",
    "    train_mse = np.array(hist['mean_squared_error'])\n",
    "    if validation_split != 0:\n",
    "        val_mse = np.array(hist['val_mean_squared_error'])\n",
    "        return train_mse, val_mse\n",
    "    return train_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1543,
   "id": "d37b06ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluation(model = None, X_test = None, y_test = None, batch_size = 10):\n",
    "    return model.evaluate(X_test, y_test, batch_size = batch_size, verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0802900f",
   "metadata": {},
   "source": [
    "#### Definindo hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1544,
   "id": "f56f14e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = {'first_layer_units': 64, 'epochs': 6, 'batch_size': 20, 'alpha': 0.001, 'reg_lambda': 0.1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504a543d",
   "metadata": {},
   "source": [
    "#### Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1545,
   "id": "b457dea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(first_layer_units = hp['first_layer_units'], alpha = hp['alpha'],\n",
    "                     reg_lambda = hp['reg_lambda'], features = features, random_seed = random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb13976",
   "metadata": {},
   "source": [
    "#### Treino e validação de nosso primeiro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1546,
   "id": "94b6fe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1547,
   "id": "e9aa64e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_mse, val_mse = train_model(model, X_train_scaled, y_train_scaled, epochs = hp['epochs'], \n",
    "                                 batch_size = hp['batch_size'], random_seed = random_state,\n",
    "                                validation_split = val_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e1f3dd",
   "metadata": {},
   "source": [
    "- Notamos um comportamento à primeira vista \"estranho\": os valores da função loss, que foi escolhida como mean_squared_error, está diferente da métrica mean_squared_error. Isso ocorre porque o cálculo de loss ocorre fazendo a média aritmética das somas dos quadrados dos erros e o cálculo da métrica ocorre fazendo a média ponderada desse valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1548,
   "id": "37c21646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006387853529304266 0.005437376908957958\n"
     ]
    }
   ],
   "source": [
    "print(train_mse[-1], val_mse[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1549,
   "id": "d8194479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006299859378486872"
      ]
     },
     "execution_count": 1549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluation(model, X_test_scaled, y_test_scaled, batch_size = hp['batch_size'])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae9bec7",
   "metadata": {},
   "source": [
    "Parece que o modelo se saiu muito bem, pois os valores das métricas nos datasets de treino, validação e de teste estão muito próximas e bastante pequenas, chegando a valores próximos daqueles que obtivemos por meio do modelo de regressão linear feito from scratch. Notemos que o valor do mse dos dados de validação é consistentemente menor que dos dados de treino. Isso nos indica que o conjunto de dados é, de alguma forma, enviesado e os dados de validação escolhidos acabam sofrendo esse viés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fced46",
   "metadata": {},
   "source": [
    "#### Agora que encontramos valores bons para os hiperparâmetros, vamos testar  a performance do modelo para vários valores de nodes na first layer do modelo, verificando como o resultado se comporta em função da complexidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1550,
   "id": "c1d19f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_values = [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1551,
   "id": "5ec0aea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_mse = {}\n",
    "val_final_mse = {}\n",
    "test_final_mse = {}\n",
    "for first_layer_unit in unit_values:\n",
    "    model = create_model(first_layer_units = first_layer_unit, alpha = hp['alpha'],\n",
    "                     reg_lambda = hp['reg_lambda'], features = features, random_seed = random_state)\n",
    "    train_mse, val_mse = train_model(model, X_train_scaled, y_train_scaled, epochs = hp['epochs'], \n",
    "                                     batch_size = hp['batch_size'], random_seed = random_state)\n",
    "    train_final_mse[first_layer_unit] = train_mse[-1]\n",
    "    val_final_mse[first_layer_unit] = val_mse[-1]\n",
    "    test_eval = test_evaluation(model, X_test_scaled, y_test_scaled, batch_size = hp['batch_size'])\n",
    "    test_final_mse[first_layer_unit] = test_eval[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1552,
   "id": "f66a308a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2:\n",
      "Train final mse: 0.28186458349227905\n",
      "Val final mse: 0.25501877069473267\n",
      "Test final mse: 0.25808054208755493\n",
      "4:\n",
      "Train final mse: 0.45445516705513\n",
      "Val final mse: 0.4187878668308258\n",
      "Test final mse: 0.4218711256980896\n",
      "8:\n",
      "Train final mse: 0.38972270488739014\n",
      "Val final mse: 0.36853405833244324\n",
      "Test final mse: 0.37934401631355286\n",
      "16:\n",
      "Train final mse: 0.016190094873309135\n",
      "Val final mse: 0.016656536608934402\n",
      "Test final mse: 0.016049453988671303\n",
      "32:\n",
      "Train final mse: 0.012281469069421291\n",
      "Val final mse: 0.011341418139636517\n",
      "Test final mse: 0.011271507479250431\n",
      "64:\n",
      "Train final mse: 0.00787552073597908\n",
      "Val final mse: 0.006427847780287266\n",
      "Test final mse: 0.007352294400334358\n",
      "128:\n",
      "Train final mse: 0.006064746994525194\n",
      "Val final mse: 0.005578806158155203\n",
      "Test final mse: 0.006258651614189148\n",
      "256:\n",
      "Train final mse: 0.005137995816767216\n",
      "Val final mse: 0.004596150945872068\n",
      "Test final mse: 0.005783664062619209\n",
      "512:\n",
      "Train final mse: 0.006234250962734222\n",
      "Val final mse: 0.003946168813854456\n",
      "Test final mse: 0.005451966542750597\n",
      "1024:\n",
      "Train final mse: 0.009519937448203564\n",
      "Val final mse: 0.007634608540683985\n",
      "Test final mse: 0.009621179662644863\n"
     ]
    }
   ],
   "source": [
    "for unit in unit_values:\n",
    "    print(f'{unit}:')\n",
    "    print(f'Train final mse: {train_final_mse[unit]}')\n",
    "    print(f'Val final mse: {val_final_mse[unit]}')\n",
    "    print(f'Test final mse: {test_final_mse[unit]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53a12d9",
   "metadata": {},
   "source": [
    "Vemos, através desse experimento, fatos curiosos:\n",
    "\n",
    "- O valor do erro final do modelo tende a ser maior se temos um modelo exageradamente complexo, enquanto que modelos não tão complexo, o modelo performa relativamente bem. Isso acontece porque quando aumentamos a complexidade do modelo, ele tende a sofrer overfitting nos dados de treino, perdendo sua capacidade de generalização.\n",
    "\n",
    "- Além disso, o valor \"ótimo\" de units é data-dependent, logo, não conseguimos prever antecipadamente qual dos valores de units será o melhor - apenas sabemos que não deve ser um valor extremamente grande, dado que estamos resolvendo um problema em que a regressão linear se encaixa muito bem devido às altas correlações das features com o target.\n",
    "\n",
    "- Parece que o melhor valor foi 128, em termos de proximidades dos erros e de valores pequenos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa910fcd",
   "metadata": {},
   "source": [
    "#### Note que as correlações entre a variável dependente (target) e as features são de fato elevadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1553,
   "id": "6a16af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_data(X, y):\n",
    "    data = X.assign(Chance_of_admit  = y)\n",
    "    data = data.rename(columns = {'Chance_of_admit': 'Chance of Admit'})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1554,
   "id": "bfb1b1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = reconstruct_data(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1555,
   "id": "a4d63d19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRE Score            0.796729\n",
       "SOP                  0.674721\n",
       "Research             0.517726\n",
       "CGPA                 0.869127\n",
       "LOR                  0.686924\n",
       "University Rating    0.713946\n",
       "TOEFL Score          0.788699\n",
       "Chance of Admit      1.000000\n",
       "Name: Chance of Admit, dtype: float64"
      ]
     },
     "execution_count": 1555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.corr()['Chance of Admit']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8c9419",
   "metadata": {},
   "source": [
    "### Modelo selecionado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11719da6",
   "metadata": {},
   "source": [
    "Para nosso modelo, escolheremos à primeira vista first_layer_units = 128, pois foi um modelo que performou bem e que mostrou poucos sinais de overfitting nos dados de treino com os hiperparâmetros selecionados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1556,
   "id": "c79c4326",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp['first_layer_units'] = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1557,
   "id": "7c1d33e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_layer_units': 128,\n",
       " 'epochs': 6,\n",
       " 'batch_size': 20,\n",
       " 'alpha': 0.001,\n",
       " 'reg_lambda': 0.1}"
      ]
     },
     "execution_count": 1557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1558,
   "id": "09eb3a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = create_model(first_layer_units = hp['first_layer_units'], alpha = hp['alpha'],\n",
    "                     reg_lambda = hp['reg_lambda'], features = features, random_seed = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1559,
   "id": "78ba7651",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_train_mse, se_val_mse = train_model(selected_model, X_train_scaled, y_train_scaled, epochs = hp['epochs'], \n",
    "                                     batch_size = hp['batch_size'], random_seed = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1560,
   "id": "99966311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for our selected linear regression model made with NN in Tensorflow Keras API:\n",
      "\n",
      "Selected model train mse: 0.006064746994525194\n",
      "Selected model val mse: 0.005578806158155203\n",
      "Selected model test mse: 0.006258651614189148\n"
     ]
    }
   ],
   "source": [
    "se_test_eval = test_evaluation(selected_model, X_test_scaled, y_test_scaled, batch_size = hp['batch_size'])\n",
    "print(f'\\nMetrics for our selected linear regression model made with NN in Tensorflow Keras API:\\n')\n",
    "print(f'Selected model train mse: {se_train_mse[-1]}')\n",
    "print(f'Selected model val mse: {se_val_mse[-1]}')\n",
    "print(f'Selected model test mse: {se_test_eval[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cbb770",
   "metadata": {},
   "source": [
    "#### Agora que temos nosso modelo, vamos analisar novamente se ele não sofre overfitting por meio de outra técnica de cross-validation: K-fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1561,
   "id": "470f2a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_layer_units': 128,\n",
       " 'epochs': 6,\n",
       " 'batch_size': 20,\n",
       " 'alpha': 0.001,\n",
       " 'reg_lambda': 0.1}"
      ]
     },
     "execution_count": 1561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1562,
   "id": "0c21665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(data, random_state):\n",
    "    '''Shuffles a Pandas Dataframe's data.'''\n",
    "    rand = np.random.RandomState(random_state)\n",
    "    return data.reindex(rand.permutation(data.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1563,
   "id": "1fc43c3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KFold: k iterations in which (k-1) splits are for train and 1 for test.\n",
    "def kfold_cross_validation(X = None, y = None, n_splits = 5, hp = {}, shuffle = True, random_state = 0):\n",
    "    # Shuffling to avoid bias\n",
    "    if shuffle:\n",
    "        features = X.columns\n",
    "        label = y.columns\n",
    "        data = reconstruct_data(X, y)\n",
    "        data = shuffle_data(data, random_state)\n",
    "        X_shuffled = data[features]\n",
    "        y_shuffled = data[label]\n",
    "        \n",
    "    kf = KFold(n_splits = n_splits)\n",
    "    k_metrics = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train = X_shuffled.iloc[train_index].copy()\n",
    "        y_train = y_shuffled.iloc[train_index].copy()\n",
    "        X_test = X_shuffled.iloc[test_index].copy()\n",
    "        y_test = y_shuffled.iloc[test_index].copy()\n",
    "        \n",
    "        # Scaling data:\n",
    "        X_train_scaled = (X_train - X_train.min())/(X_train.max() - X_train.min())\n",
    "        X_test_scaled = (X_test - X_train.min())/(X_train.max() - X_train.min())\n",
    "        y_train_scaled = y_train\n",
    "        y_test_scaled = y_test\n",
    "\n",
    "        # Creating selected model:\n",
    "        selected_model = create_model(first_layer_units = hp['first_layer_units'], alpha = hp['alpha'],\n",
    "                                        reg_lambda = hp['reg_lambda'], features = features, \n",
    "                                        random_seed = random_state)\n",
    "        se_train_mse = train_model(selected_model, X_train_scaled, y_train_scaled, epochs = hp['epochs'], \n",
    "                                  batch_size = hp['batch_size'], random_seed = random_state,\n",
    "                                  validation_split = 0)\n",
    "        se_test_eval = test_evaluation(selected_model, X_test_scaled, y_test_scaled, batch_size = hp['batch_size'])\n",
    "        se_metrics_list = [se_train_mse[-1], se_test_eval[1]]\n",
    "        k_metrics.append(se_metrics_list)\n",
    "    return k_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1564,
   "id": "757b8ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "k_metrics = kfold_cross_validation(X, y, n_splits = n_splits, shuffle = True,\n",
    "                                   random_state = random_state, hp = hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1565,
   "id": "4bb4ac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_k_metrics(k_metrics):\n",
    "    k = 0\n",
    "    for metrics in k_metrics:\n",
    "        print(f'\\nMetrics for fold {k + 1}:\\n')\n",
    "        print(f'Selected model train mse: {metrics[0]}')\n",
    "        print(f'Selected model test mse: {metrics[1]}')\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1566,
   "id": "1425fd09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for fold 1:\n",
      "\n",
      "Selected model train mse: 0.005074808839708567\n",
      "Selected model test mse: 0.005736889783293009\n",
      "\n",
      "Metrics for fold 2:\n",
      "\n",
      "Selected model train mse: 0.004938953556120396\n",
      "Selected model test mse: 0.006124397274106741\n",
      "\n",
      "Metrics for fold 3:\n",
      "\n",
      "Selected model train mse: 0.005400301888585091\n",
      "Selected model test mse: 0.0045867436565458775\n",
      "\n",
      "Metrics for fold 4:\n",
      "\n",
      "Selected model train mse: 0.005435791797935963\n",
      "Selected model test mse: 0.004165829159319401\n",
      "\n",
      "Metrics for fold 5:\n",
      "\n",
      "Selected model train mse: 0.005468782503157854\n",
      "Selected model test mse: 0.00477219745516777\n"
     ]
    }
   ],
   "source": [
    "print_k_metrics(k_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbe9a8a",
   "metadata": {},
   "source": [
    "#### Os resultados do KFold estão mostrando que ainda ainda há um certo grau de overfitting, então podemos tentar melhorar esse resultado mudando alguns parâmetros.\n",
    "\n",
    "Podemos tentar:\n",
    "\n",
    "- Reduzir alpha\n",
    "- Reduzir epochs\n",
    "- Reduzir batch_size\n",
    "- Aumentar reg_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1567,
   "id": "cd92343f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_layer_units': 128,\n",
       " 'epochs': 6,\n",
       " 'batch_size': 20,\n",
       " 'alpha': 0.001,\n",
       " 'reg_lambda': 0.1}"
      ]
     },
     "execution_count": 1567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1568,
   "id": "2b2ddf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp['reg_lambda'] = 0.16\n",
    "hp['batch_size'] = 15\n",
    "hp['alpha'] = 0.001/2\n",
    "hp['epochs'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1569,
   "id": "400c685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_metrics = kfold_cross_validation(X, y, n_splits = n_splits, shuffle = True,\n",
    "                                   random_state = random_state, hp = hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1570,
   "id": "48e000a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for fold 1:\n",
      "\n",
      "Selected model train mse: 0.006748770363628864\n",
      "Selected model test mse: 0.005964679177850485\n",
      "\n",
      "Metrics for fold 2:\n",
      "\n",
      "Selected model train mse: 0.00663524866104126\n",
      "Selected model test mse: 0.00767184142023325\n",
      "\n",
      "Metrics for fold 3:\n",
      "\n",
      "Selected model train mse: 0.0068711512722074986\n",
      "Selected model test mse: 0.005178166553378105\n",
      "\n",
      "Metrics for fold 4:\n",
      "\n",
      "Selected model train mse: 0.006851423531770706\n",
      "Selected model test mse: 0.005149665288627148\n",
      "\n",
      "Metrics for fold 5:\n",
      "\n",
      "Selected model train mse: 0.006702333688735962\n",
      "Selected model test mse: 0.005697226617485285\n"
     ]
    }
   ],
   "source": [
    "print_k_metrics(k_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1571,
   "id": "e201418d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_layer_units': 128,\n",
       " 'epochs': 5,\n",
       " 'batch_size': 15,\n",
       " 'alpha': 0.0005,\n",
       " 'reg_lambda': 0.16}"
      ]
     },
     "execution_count": 1571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be286929",
   "metadata": {},
   "source": [
    "Apesar de tentarmos várias vezes mudar os hiperparâmetros, para o fold 2 não conseguimos aproximar o test mse do train mse. Além disso, há comportamentos anormais em todos os outros folds: os valores do train mse são maiores que do test mse. Isso indica que há viés nos dados, especialmente nesses folds. Uma maneira de tentarmos contornar esse problema seria aumentar o número de camadas do modelo ou o número de nodes por camada. Podemos também adicionar uma camada de dropout para evitar overfitting com o aumento de complexidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1589,
   "id": "ee9151d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(first_layer_units = 32, alpha = 0.1, reg_lambda = 0.1, dropout_rate = 0, features = [], random_seed = 0):\n",
    "    '''Create a defined model with learning rate = alpha.'''\n",
    "    # Defining Tensorflow's random seed to generate reproducible results.\n",
    "    tf.keras.utils.set_random_seed(random_seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "    \n",
    "    # We will be using Tensorflow's Sequencial API to build our model.\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # Adding first layer with input_dim = number of features and relu activation function \n",
    "    # to learn some non-linearities. We will also add L2 regularization in layer's kernel (weights).\n",
    "    model.add(tf.keras.layers.Dense(units = first_layer_units, input_shape = (len(features), ), \n",
    "                                   kernel_regularizer = tf.keras.regularizers.L2(l2 = reg_lambda),\n",
    "                                   activation = 'relu'))\n",
    "    \n",
    "    # Dropout layer\n",
    "    \n",
    "    model.add(tf.keras.layers.Dropout(rate = dropout_rate, seed = random_seed))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(units = first_layer_units, input_shape = (len(features), ), \n",
    "                                    kernel_regularizer = tf.keras.regularizers.L2(l2 = reg_lambda)))\n",
    "    \n",
    "    # Adding output layer with linear activation (linear regression output).\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(units = 1, activation = 'linear'))\n",
    "    \n",
    "    # Compiling model: selecting optimizer, loss and metrics.\n",
    "    \n",
    "    model.compile(optimizer = tf.keras.optimizers.RMSprop(learning_rate = alpha),\n",
    "                 loss = 'mean_squared_error',\n",
    "                 metrics = tf.keras.metrics.MeanSquaredError())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1595,
   "id": "c1d4ffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFold: k iterations in which (k-1) splits are for train and 1 for test.\n",
    "def kfold_cross_validation(X = None, y = None, n_splits = 5, hp = {}, shuffle = True, random_state = 0):\n",
    "    # Shuffling to avoid bias\n",
    "    if shuffle:\n",
    "        features = X.columns\n",
    "        label = y.columns\n",
    "        data = reconstruct_data(X, y)\n",
    "        data = shuffle_data(data, random_state)\n",
    "        X_shuffled = data[features]\n",
    "        y_shuffled = data[label]\n",
    "        \n",
    "    kf = KFold(n_splits = n_splits)\n",
    "    k_metrics = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train = X_shuffled.iloc[train_index].copy()\n",
    "        y_train = y_shuffled.iloc[train_index].copy()\n",
    "        X_test = X_shuffled.iloc[test_index].copy()\n",
    "        y_test = y_shuffled.iloc[test_index].copy()\n",
    "        \n",
    "        # Scaling data:\n",
    "        X_train_scaled = (X_train - X_train.min())/(X_train.max() - X_train.min())\n",
    "        X_test_scaled = (X_test - X_train.min())/(X_train.max() - X_train.min())\n",
    "        y_train_scaled = y_train\n",
    "        y_test_scaled = y_test\n",
    "\n",
    "        # Creating selected model:\n",
    "        selected_model = create_model(first_layer_units = hp['first_layer_units'], alpha = hp['alpha'],\n",
    "                                        reg_lambda = hp['reg_lambda'], features = features, \n",
    "                                        random_seed = random_state, dropout_rate = hp['dropout_rate'])\n",
    "        se_train_mse = train_model(selected_model, X_train_scaled, y_train_scaled, epochs = hp['epochs'], \n",
    "                                  batch_size = hp['batch_size'], random_seed = random_state,\n",
    "                                  validation_split = 0)\n",
    "        se_test_eval = test_evaluation(selected_model, X_test_scaled, y_test_scaled, batch_size = hp['batch_size'])\n",
    "        se_metrics_list = [se_train_mse[-1], se_test_eval[1]]\n",
    "        k_metrics.append(se_metrics_list)\n",
    "    return k_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1644,
   "id": "eb21bae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp['reg_lambda'] = 0.16\n",
    "hp['batch_size'] = 15\n",
    "hp['alpha'] = 0.001/2\n",
    "hp['epochs'] = 5\n",
    "hp['dropout_rate'] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1645,
   "id": "5758c6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_layer_units': 128,\n",
       " 'epochs': 5,\n",
       " 'batch_size': 15,\n",
       " 'alpha': 0.0005,\n",
       " 'reg_lambda': 0.16,\n",
       " 'dropout_rate': 0.1}"
      ]
     },
     "execution_count": 1645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1646,
   "id": "e1846ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_metrics = kfold_cross_validation(X, y, n_splits = n_splits, shuffle = True,\n",
    "                                   random_state = random_state, hp = hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1647,
   "id": "851628f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for fold 1:\n",
      "\n",
      "Selected model train mse: 0.00832164753228426\n",
      "Selected model test mse: 0.009839721024036407\n",
      "\n",
      "Metrics for fold 2:\n",
      "\n",
      "Selected model train mse: 0.007600715849548578\n",
      "Selected model test mse: 0.007761502172797918\n",
      "\n",
      "Metrics for fold 3:\n",
      "\n",
      "Selected model train mse: 0.008036257699131966\n",
      "Selected model test mse: 0.006805111654102802\n",
      "\n",
      "Metrics for fold 4:\n",
      "\n",
      "Selected model train mse: 0.008295999839901924\n",
      "Selected model test mse: 0.007244655396789312\n",
      "\n",
      "Metrics for fold 5:\n",
      "\n",
      "Selected model train mse: 0.008186479099094868\n",
      "Selected model test mse: 0.006620819214731455\n"
     ]
    }
   ],
   "source": [
    "print_k_metrics(k_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f60e2",
   "metadata": {},
   "source": [
    "- Note que adicionar novas camadas ao modelo, mesmo que ajustemos alguns parâmetros, aumentou o erro obtido e ainda não auxiliou na diminuição da diferença entre os mse de treino e de teste em geral, pois o fold 1 ficou agora com uma grande diferença. Isso indica mais ainda que, em determinados folds, há, de fato, dados enviesados, mesmo que tenhamos embaralhado anteriormente o dataset. \n",
    "\n",
    "- O ideal nesse caso era termos mais dados para conseguirmos, com sorte, um modelo melhor.\n",
    "\n",
    "- Nesse dataset em específico, aparentemente o modelo mais simples performou melhor, apesar dos comportamentos estranhos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
