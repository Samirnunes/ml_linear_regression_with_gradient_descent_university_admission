{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75ea5e69",
   "metadata": {},
   "source": [
    "# Machine Learning: Gradiente Descendente Estocástico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238dcd9c",
   "metadata": {},
   "source": [
    "### Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eab28aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(0) # Para termos sempre os mesmos números aleatórios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5a5555",
   "metadata": {},
   "source": [
    "Utilizaremos o dataset \"Data for Admission in the University\", disponível no Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab953b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('adm_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f01671",
   "metadata": {},
   "source": [
    "### Visualizando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47463ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "280218d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Serial No.', 'GRE Score', 'TOEFL Score', 'University Rating', 'SOP',\n",
       "       'LOR ', 'CGPA', 'Research', 'Chance of Admit '],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d74f480",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {'Chance of Admit ': 'Chance of Admit'}, inplace = True)\n",
    "data.rename(columns = {'LOR ': 'LOR'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60948456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR  CGPA  \\\n",
       "0           1        337          118                  4  4.5  4.5  9.65   \n",
       "1           2        324          107                  4  4.0  4.5  8.87   \n",
       "2           3        316          104                  3  3.0  3.5  8.00   \n",
       "3           4        322          110                  3  3.5  2.5  8.67   \n",
       "4           5        314          103                  2  2.0  3.0  8.21   \n",
       "\n",
       "   Research  Chance of Admit  \n",
       "0         1             0.92  \n",
       "1         1             0.76  \n",
       "2         1             0.72  \n",
       "3         1             0.80  \n",
       "4         0             0.65  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3f08ae",
   "metadata": {},
   "source": [
    "#### Definições:\n",
    "\n",
    "__Serial No.:__ número de série do estudante (índice + 1)\n",
    "\n",
    "__GRE Score:__ pontuação do estudante no GRE (Graduate Record Examination), um exame padronizado semelhante ao GMAT.\n",
    "\n",
    "__TOEFL Score:__ pontuação no TOEFL, um exame completo de inglês utilizado, entre outras coisas, para admissão em universidades.\n",
    "\n",
    "__University Rating:__ avaliação da universidade (quanto maior, mais conceituada é a universidade na qual o estudante quer entrar).\n",
    "\n",
    "__SOP:__ avaliação do Statement of Purpose, uma redação explicando o propósito do estudante ao aplicar para uma vaga em uma dada graduação em uma universidade.\n",
    "\n",
    "__LOR:__ avaliação da Letter of Recommendation, a carta de recomendação do estudante para a universidade.\n",
    "\n",
    "__CGPA:__ Cumulative Grade Point Average, é uma pontuação utilizada para medir o desempenho médio de um estudante.\n",
    "\n",
    "__Research:__ experiência em pesquisa (1 se o estudante tiver, 0 se não).\n",
    "\n",
    "__Chance of Admit:__ chance de admissão na universidade, indo de 0 a 1 (100%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5787262d",
   "metadata": {},
   "source": [
    "### Definição da label e das features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6666d4",
   "metadata": {},
   "source": [
    "Nossa label (y) será a Chance of Admit.\n",
    "\n",
    "A lista de colunas [\"GRE Score\", \"TOEFL Score\", \"University Rating\", \"SOP\", \"LOR\", \"CGPA\", \"Research\"] contém as features altamente correlacionadas com a chance de admissão na universidade. No nosso caso, iremos tratar apenas das features contínuas, sendo a lista de features que pode ser utilizada a seguinte: features = [\"GRE Score\", \"TOEFL Score\", \"University Rating\", \"SOP\", \"LOR\", \"CGPA\"].\n",
    "\n",
    "Implementaremos o algoritmo de __Gradiente Descendente Estocástico em mini-lotes__  nos seguintes casos:\n",
    "\n",
    "1) Modelo de Regressão Linear com 1 feature.\n",
    "\n",
    "2) Modelo de Regressão Linear com todas as features contínuas do dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9218ea7f",
   "metadata": {},
   "source": [
    "### 1) Modelo de Regressão Linear com 1 feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6682d1",
   "metadata": {},
   "source": [
    "O modelo será da forma: \n",
    "\n",
    "$\\hat{y} = w_{1}x_{1} + b$\n",
    "\n",
    "Em que $\\hat{y}$ é a estimativa (ou predição) da label $y$, dado um valor da feature $x_{1}$, $w_{1}$ é o peso (weight) associado à feature $x_{1}$ e $b$ é chamado de viés (bias)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6083a440",
   "metadata": {},
   "source": [
    "Vamos definir a perda (loss) associada ao modelo através da função de perda $L_{2}$, também chamada de squared loss. Assim:\n",
    "\n",
    "$loss = \\frac{1}{n}\\sum_{i = 1}^{n} (\\hat{y_{i}} - y_{i})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d32012d",
   "metadata": {},
   "source": [
    "Em que n é o número de dados que iremos utilizar no aprendizado de nosso modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eaf8f2",
   "metadata": {},
   "source": [
    "Podemos escrevê-la ainda como função explícita dos parâmetros $w_{1}$ e $b$:\n",
    "\n",
    "$loss = f(w_{1}, b) = \\frac{1}{n}\\sum_{i = 1}^{n} (w_{1}(x_{1})_{i} + b - y_{i})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d4265",
   "metadata": {},
   "source": [
    "Como queremos reduzir a perda de nosso modelo ao máximo, devemos fazer os parâmetros variarem na direção do  negativo do gradiente de f (esse é o princípio do algoritmo de Gradiente Descendente Estocástico). \n",
    "\n",
    "Nesse contexto, chamemos $\\theta = (w_{1}, b)$. Além disso, definimos o hiperparâmetro $\\alpha$ como sendo a taxa de aprendizagem do modelo, a qual afeta diretamente a velocidade de convergência do modelo para os parâmetros ideais.\n",
    "\n",
    "Assim, o novo valor de $\\theta$, $\\theta'$, será dado pela fórmula:\n",
    "\n",
    "$\\theta' = \\theta - \\alpha \\cdot \\nabla f(w_{1}, b)$\n",
    "\n",
    "Onde $\\nabla f(w_{1}, b) = (\\frac{\\partial f}{\\partial w_{1}}, \\frac{\\partial f}{\\partial b})$ é o gradiente de f."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afe6a26",
   "metadata": {},
   "source": [
    "Calculando as derivadas parciais:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial w_{1}} = \\frac{2}{n}\\sum_{i = 1}^{n} (w_{1}(x_{1})_{i} + b - y_{i}) \\cdot (x_{1})_{i}$\n",
    "\n",
    "$\\frac{\\partial f}{\\partial b} = \\frac{2}{n}\\sum_{i = 1}^{n} (w_{1}(x_{1})_{i} + b - y_{i})$\n",
    "\n",
    "Obtemos os novos valores do peso e do viés após uma iteração:\n",
    "\n",
    "$w_{1}' = w_{1} - \\frac{2 \\alpha}{n}\\sum_{i = 1}^{n} (w_{1}(x_{1})_{i} + b - y_{i}) \\cdot (x_{1})_{i}$\n",
    "\n",
    "$b' = b - \\frac{2 \\alpha}{n}\\sum_{i = 1}^{n} (w_{1}(x_{1})_{i} + b - y_{i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410cece1",
   "metadata": {},
   "source": [
    "Com isso em mente, iremos implementar o modelo de regressão linear que se adequa aos dados através do Gradiente Descendente Estocástico em mini-lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39e3ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Regression_Model():\n",
    "    def __init__(self, data, feature_name: str, label_name: str, w1: float, b: float, alpha: float):\n",
    "        self.feature = data[feature_name]\n",
    "        self.label = data[label_name]\n",
    "        self.w1 = w1\n",
    "        self.b = b\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def print_parameters(self):\n",
    "        print(f'w1 = {self.w1}\\nb = {self.b}')\n",
    "        \n",
    "    def get_prediction(self, x1: float):\n",
    "        return self.w1*x1 + self.b\n",
    "        \n",
    "    def get_loss(self):\n",
    "        loss = 0\n",
    "        n = len(self.label)\n",
    "        for (x1, y) in zip(self.feature, self.label):\n",
    "            loss += (1/n)*(self.get_prediction(x1) - y)**2\n",
    "        return loss\n",
    "        \n",
    "    def sgd_update_parameters(self, batch_size: int):\n",
    "        index_list = list(range(0, len(self.label)))\n",
    "        random_indices = np.random.choice(index_list, size = batch_size, replace = True) # bootstrap sample\n",
    "        x1_sample = self.feature.iloc[random_indices]\n",
    "        y_sample = self.label.iloc[random_indices]\n",
    "        old_w1 = self.w1\n",
    "        old_b = self.b\n",
    "        for (x1, y) in zip(x1_sample, y_sample):\n",
    "            partial_w1 = (2/batch_size) * (old_w1*x1 + old_b - y) * x1\n",
    "            partial_b = (2/batch_size) * (old_w1*x1 + old_b - y)\n",
    "            self.w1 -= self.alpha * partial_w1\n",
    "            self.b -= self.alpha * partial_b\n",
    "        \n",
    "    def sgd(self, iterations: int, batch_size: float, print_loss: bool): # stochastic gradient descent\n",
    "        for i in range(0, iterations):\n",
    "            self.sgd_update_parameters(batch_size)\n",
    "            if print_loss:\n",
    "                print(f'loss = {self.get_loss()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2455c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = 'CGPA'\n",
    "label_name = 'Chance of Admit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d2844d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Linear_Regression_Model(data = data, feature_name = feature_name, label_name = label_name, w1 = 0.2, b = -1, alpha = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f1faf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sgd(iterations = 10000, batch_size = 50, print_loss = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20e93c89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 = 0.20625614383688787\n",
      "b = -1.0437656212134423\n"
     ]
    }
   ],
   "source": [
    "model.print_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba0037d",
   "metadata": {},
   "source": [
    "### Plotando a reta de regressão obtida através do SGD e comparando com a curva analítica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de54bd1a",
   "metadata": {},
   "source": [
    "#### Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8fdeb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec631380",
   "metadata": {},
   "source": [
    "#### Obtendo os parâmetros da curva analítica através do numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4eee01a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_np, b_np = np.polyfit(data[feature_name], data[label_name], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fb16ccd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20884722950069112"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "238935e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.071511662934231"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396b500a",
   "metadata": {},
   "source": [
    "#### Obtendo os pontos da reta SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1327dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 10.01, 0.01)\n",
    "y_sgd = []\n",
    "for x1 in x:\n",
    "    y_sgd.append(model.get_prediction(x1))\n",
    "y_sgd = np.array(y_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c899869d",
   "metadata": {},
   "source": [
    "#### Plot das retas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a95674fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x21a0baf2fa0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(data[feature_name], data[label_name], 'o', color = 'gray') # Plot dos dados\n",
    "plt.plot(x, y_sgd, color = 'orange') # Plot da reta SGD\n",
    "plt.plot(x, m_np*x + b_np, color = 'red') # Plot da reta de regressão analítica\n",
    "plt.xlim([7.0, 10.0])\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(['Data', 'SGD', 'Analytical'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a1e1f",
   "metadata": {},
   "source": [
    "### 2) Modelo de Regressão Linear com todas as features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7908d7",
   "metadata": {},
   "source": [
    "Com n features, o modelo será da forma:\n",
    "\n",
    "$\\hat{y} = w_{1}x_{1} + w_{2}x_{2} + ... + w_{n}x_{n} + b$\n",
    "\n",
    "Em que $\\hat{y}$ é a estimativa (ou predição) da label $y$, dados os valores das features $x_{1}, x_{2},...,x_{n}$, $(w_{1}, w_{2},...,w_{n})$ é o conjunto de pesos (weight) associado às features e $b$ é chamado de viés (bias)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e02f33",
   "metadata": {},
   "source": [
    "Vamos definir a perda (loss) associada ao modelo através da função de perda $L_{2}$, da mesma forma que foi feito no primeiro modelo. Assim:\n",
    "\n",
    "$loss = \\frac{1}{n}\\sum_{i = 1}^{n} (\\hat{y_{i}} - y_{i})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ea535c",
   "metadata": {},
   "source": [
    "Nesse contexto, chamemos $\\theta = (w_{1}, w_{2}, ... , w_{n}, b)$. Além disso, definimos o hiperparâmetro $\\alpha$ como a taxa de aprendizagem do modelo, de forma análoga ao modelo anterior.\n",
    "\n",
    "Assim, por meio do gradiente descendente estocástico, o novo valor de $\\theta$, $\\theta'$, será dado pela fórmula:\n",
    "\n",
    "$\\theta' = \\theta - \\alpha \\cdot \\nabla f(w_{1}, w_{2}, ... , w_{n}, b)$\n",
    "\n",
    "Onde $\\nabla f(w_{1}, w_{2}, ... , w_{n}, b) = (\\frac{\\partial f}{\\partial w_{1}}, \\frac{\\partial f}{\\partial w_{2}},..., \\frac{\\partial f}{\\partial w_{n}}, \\frac{\\partial f}{\\partial b})$ é o gradiente de f."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64128bc",
   "metadata": {},
   "source": [
    "Calculando as derivadas parciais:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial w_{k}} = \\frac{2}{n}\\sum_{i = 1}^{n} (\\hat{y}_{i} - y_{i}) \\cdot (x_{k})_{i}$, para toda feature $x_{k}$.\n",
    "\n",
    "$\\frac{\\partial f}{\\partial b} = \\frac{2}{n}\\sum_{i = 1}^{n} (\\hat{y}_{i} - y_{i})$\n",
    "\n",
    "Obtemos os novos valores do peso e do viés após uma iteração:\n",
    "\n",
    "$w_{k}' = w_{k} - \\frac{2 \\alpha}{n}\\sum_{i = 1}^{n} (\\hat{y}_{i} - y_{i}) \\cdot (x_{k})_{i}$, para toda feature $x_{k}$.\n",
    "\n",
    "\n",
    "$b' = b - \\frac{2 \\alpha}{n}\\sum_{i = 1}^{n} (\\hat{y}_{i} - y_{i})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83e8c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Features_Linear_Regression_Model():\n",
    "    def __init__(self, train_data, features_name: list, label_name: str, ws: list, b: float, alpha: float, random_state: int):\n",
    "        self.train_data = train_data\n",
    "        self.features = train_data[features_name]\n",
    "        self.label = train_data[label_name]\n",
    "        self.label_name = label_name # saving here because label as a series doesn't save it.\n",
    "        for i in range(0, len(ws)):\n",
    "            ws[i] = float(ws[i])\n",
    "        self.ws = np.array(ws) # weights\n",
    "        self.b = b\n",
    "        self.alpha = alpha\n",
    "        self.rand = np.random.RandomState(random_state)\n",
    "        \n",
    "    def print_parameters(self):\n",
    "        for i in range(1, len(self.ws) + 1):\n",
    "            print(f'w{i} = {self.ws[i - 1]}')\n",
    "        print (f'b = {self.b}')\n",
    "        \n",
    "    def get_single_prediction(self, xs: list):\n",
    "        '''Get the prediction for a list with all the features' values.'''\n",
    "        for i in range(0, len(xs)):\n",
    "            xs[i] = float(xs[i])\n",
    "        xs = np.array(xs)\n",
    "        pred = np.dot(self.ws, xs) + self.b\n",
    "        return pred\n",
    "    \n",
    "    def predict(self, data):\n",
    "        n = len(data)\n",
    "        features_name = self.features.columns\n",
    "        data_features = data[features_name]\n",
    "        predictions = np.zeros(n)\n",
    "        for i in range(0, n):\n",
    "            xs = list(data_features.iloc[i])\n",
    "            predictions[i] += self.get_single_prediction(xs)\n",
    "        return predictions\n",
    "    \n",
    "    def get_loss(self, data):\n",
    "        n = len(data)\n",
    "        data_label = data[self.label_name]\n",
    "        y = np.array(data_label)\n",
    "        predictions = self.predict(data)\n",
    "        diff = predictions - y\n",
    "        loss = (1/n)*np.dot(diff, diff)\n",
    "        return loss\n",
    "               \n",
    "    def sgd_update_parameters(self, batch_size: int):\n",
    "        n = len(self.label)\n",
    "        index_list = list(range(0, n))\n",
    "        random_indices = self.rand.choice(index_list, size = batch_size, replace = True) # bootstrap sample\n",
    "        xs_sample = list()\n",
    "        y_sample = np.array(self.label.iloc[random_indices])\n",
    "        preds_sample = np.zeros(batch_size)\n",
    "        for i in range(0, batch_size):\n",
    "            xs = list(self.features.iloc[random_indices[i]])\n",
    "            preds_sample[i] += self.get_single_prediction(xs)\n",
    "        for col in self.features:\n",
    "            xs_sample.append(np.array(self.features[col].iloc[random_indices])) # len(xs_sample) = len(self.ws)\n",
    "        diff_sample = preds_sample - y_sample\n",
    "        partial_w = np.zeros(len(self.ws))\n",
    "        for i in range(0, len(self.ws)):\n",
    "            partial_w[i] += (2/batch_size) * np.dot(diff_sample, xs_sample[i])\n",
    "        partial_b = (2/batch_size) * np.sum(diff_sample)\n",
    "        self.ws -= self.alpha * partial_w\n",
    "        self.b -= self.alpha * partial_b\n",
    "        \n",
    "    def sgd(self, iterations: int, batch_size: float, print_loss: bool): # stochastic gradient descent\n",
    "        for i in range(0, iterations):\n",
    "            self.sgd_update_parameters(batch_size)\n",
    "            if print_loss:\n",
    "                print(f'loss = {self.get_loss(self.train_data)}')\n",
    "    \n",
    "    @staticmethod\n",
    "    def shuffle_data(data, random_state):\n",
    "        rand = np.random.RandomState(random_state)\n",
    "        return data.reindex(rand.permutation(data.index))\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_val_test_split(data, test_split: float, val_split: float):\n",
    "        '''Get train, validation and test dataframes from data.'''\n",
    "        n = len(data)\n",
    "        test_size = int(test_split * n)\n",
    "        val_size = int(val_split * n)\n",
    "        test_data = data.iloc[list(range(0, test_size))]\n",
    "        val_data = data.iloc[list(range(test_size, test_size + val_size))]\n",
    "        train_data = data.iloc[list(range(test_size + val_size, n))]\n",
    "        return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55abfa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"GRE Score\", \"TOEFL Score\", \"University Rating\", \"SOP\", \"LOR\", \"CGPA\"]\n",
    "label = 'Chance of Admit'\n",
    "ws = [0, 0, 0, 0, 0, 0]\n",
    "b = 0\n",
    "alpha = 10**(-6)\n",
    "multi_features_model = Multi_Features_Linear_Regression_Model(train_data = data, features_name = features,\n",
    "                                                 label_name = label, ws = ws, b = b, alpha = alpha,\n",
    "                                                 random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7da95491",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 = 0.0\n",
      "w2 = 0.0\n",
      "w3 = 0.0\n",
      "w4 = 0.0\n",
      "w5 = 0.0\n",
      "w6 = 0.0\n",
      "b = 0\n"
     ]
    }
   ],
   "source": [
    "multi_features_model.print_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1c06f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_features_model.sgd(iterations = 1000, batch_size = 50, print_loss = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f75c961a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014474772444605608"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_features_model.get_loss(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11d3c5fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 = 0.0019295091599966833\n",
      "w2 = 0.001045757153984496\n",
      "w3 = 0.00018725309230324785\n",
      "w4 = 0.00015745378163289423\n",
      "w5 = 0.00014084973545750904\n",
      "w6 = 0.0001181198175654461\n",
      "b = -1.6019755666874852e-07\n"
     ]
    }
   ],
   "source": [
    "multi_features_model.print_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aab0d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(data[label])\n",
    "pred = np.zeros(n)\n",
    "pred = multi_features_model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a6be79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_pred = data.assign(Predictions = pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "360d9899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.776875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.740117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.720956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.738824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.715660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>396</td>\n",
       "      <td>324</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>1</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.742868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>397</td>\n",
       "      <td>325</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.741589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>398</td>\n",
       "      <td>330</td>\n",
       "      <td>116</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.45</td>\n",
       "      <td>1</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.761332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>399</td>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.78</td>\n",
       "      <td>0</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.712433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>400</td>\n",
       "      <td>333</td>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.66</td>\n",
       "      <td>1</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.768121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR  CGPA  \\\n",
       "0             1        337          118                  4  4.5  4.5  9.65   \n",
       "1             2        324          107                  4  4.0  4.5  8.87   \n",
       "2             3        316          104                  3  3.0  3.5  8.00   \n",
       "3             4        322          110                  3  3.5  2.5  8.67   \n",
       "4             5        314          103                  2  2.0  3.0  8.21   \n",
       "..          ...        ...          ...                ...  ...  ...   ...   \n",
       "395         396        324          110                  3  3.5  3.5  9.04   \n",
       "396         397        325          107                  3  3.0  3.5  9.11   \n",
       "397         398        330          116                  4  5.0  4.5  9.45   \n",
       "398         399        312          103                  3  3.5  4.0  8.78   \n",
       "399         400        333          117                  4  5.0  4.0  9.66   \n",
       "\n",
       "     Research  Chance of Admit  Predictions  \n",
       "0           1             0.92     0.776875  \n",
       "1           1             0.76     0.740117  \n",
       "2           1             0.72     0.720956  \n",
       "3           1             0.80     0.738824  \n",
       "4           0             0.65     0.715660  \n",
       "..        ...              ...          ...  \n",
       "395         1             0.82     0.742868  \n",
       "396         1             0.84     0.741589  \n",
       "397         1             0.91     0.761332  \n",
       "398         0             0.67     0.712433  \n",
       "399         1             0.95     0.768121  \n",
       "\n",
       "[400 rows x 10 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
